{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19044a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets==2.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f52e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from utils import get_labels\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba601e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "    def __repr__(self): # json 형식의 문자열로 변환\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self): # 클래스의 속성들을 dictionary 형태로 반환\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self): # dictionary를 json으로 변환\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ddb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "    def __repr__(self): \n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771fed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverNerProcessor(object):\n",
    "    \"\"\"Processor for the Naver NER data set \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.labels_lst = get_labels(args) # \n",
    "\n",
    "    @classmethod\n",
    "    def _read_file(cls, input_file):\n",
    "        \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = []\n",
    "            for line in f:\n",
    "                lines.append(line.strip())\n",
    "            return lines\n",
    "\n",
    "    def _create_examples(self, dataset, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, data) in enumerate(dataset):\n",
    "            words, labels = data.split('\\t')\n",
    "            words = list(words) # words.split()\n",
    "            labels = labels.split()\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "\n",
    "            labels_idx = []\n",
    "            for label in labels:\n",
    "                labels_idx.append(self.labels_lst.index(label) if label in self.labels_lst else self.labels_lst.index(\"UNK\"))\n",
    "                # event, timex3 중 원하는 label이 아니면 UNK 토큰 사용\n",
    "\n",
    "            assert len(words) == len(labels_idx)\n",
    "            # BIO태깅이 비정상적으로 되어 있는 데이터가 들어올 경우 오류 발생\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                logger.info(data)\n",
    "                # 10000개 단위로 log 출력\n",
    "            examples.append(InputExample(guid=guid, words=words, labels=labels_idx))\n",
    "            # guid: set_type(train, dev, test) - index,  words: tokenzied_text,  labels: 해당 word의 tag\n",
    "        return examples\n",
    "\n",
    "    def get_examples(self, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode: train, dev, test\n",
    "        \"\"\"\n",
    "        file_to_read = None\n",
    "        if mode == 'train':\n",
    "            file_to_read = self.args[\"train_file\"]\n",
    "        elif mode == 'dev':\n",
    "            file_to_read = self.args[\"val_file\"]\n",
    "        elif mode == 'test':\n",
    "            file_to_read = self.args[\"test_file\"]\n",
    "\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(self.args[\"data_dir\"], file_to_read)))\n",
    "        return self._create_examples(self._read_file(os.path.join(self.args[\"data_dir\"], file_to_read)), mode)\n",
    "    # log를 출력하면서 _create_examples() 메소드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbeead56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, max_seq_len, tokenizer,\n",
    "                                 pad_token_label_id=-100,\n",
    "                                 cls_token_segment_id=0,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    # Setting based on the current model type\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    unk_token = tokenizer.unk_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    " \n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 5000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "            # 진행도를 나타내는 log 출력\n",
    "\n",
    "        # Tokenize word by word (for NER)\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, slot_label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            label_ids.extend([int(slot_label)] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "            # \n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_len - special_tokens_count)]\n",
    "            label_ids = label_ids[: (max_seq_len - special_tokens_count)]\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        label_ids = [pad_token_label_id] + label_ids\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        label_ids = label_ids + ([pad_token_label_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_ids), max_seq_len)\n",
    "        assert len(label_ids) == max_seq_len, \"Error with slot labels length {} vs {}\".format(len(label_ids), max_seq_len)\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % example.guid)\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
    "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
    "            logger.info(\"label: %s \" % \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids,\n",
    "                          label_ids=label_ids\n",
    "                          ))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147faef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"naver-ner\": NaverNerProcessor,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bcdaca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, mode, use_cache=True):\n",
    "    processor = processors[args[\"task\"]](args)\n",
    "\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_file_name = 'cached_{}_{}_{}_{}'.format(\n",
    "        args[\"task\"], list(filter(None, args[\"model_name_or_path\"].split(\"/\"))).pop(), args[\"max_seq_len\"], mode)\n",
    "\n",
    "    pad_token_label_id = torch.nn.CrossEntropyLoss().ignore_index\n",
    "    cached_features_file = os.path.join(args[\"data_dir\"], cached_file_name)\n",
    "    if os.path.exists(cached_features_file) and use_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args[\"data_dir\"])\n",
    "        if mode == \"train\":\n",
    "            examples = processor.get_examples(\"train\")\n",
    "        elif mode == \"dev\":\n",
    "            examples = processor.get_examples(\"dev\")\n",
    "        elif mode == \"test\":\n",
    "            examples = processor.get_examples(\"test\")\n",
    "        else:\n",
    "            raise Exception(\"For mode, Only train, dev, test is available\")\n",
    "\n",
    "        features = convert_examples_to_features(examples, args[\"max_seq_len\"], tokenizer, pad_token_label_id=pad_token_label_id)\n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488b6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
